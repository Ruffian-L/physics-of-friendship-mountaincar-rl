# The Physics of Friendship: MountainCar Q-SMA

> **An exploration of how habits, topology, memory, and physics combine to teach an agent to escape a valley.**

This is a research project exploring reinforcement learning on the MountainCar-v0 environment using **Q-SMA** (Q-Learning + Sensory-Motor Attunement) â€” a hybrid architecture that blends classical RL with biologically-inspired systems: habit formation (Flux), topological self-monitoring (TDA), dream replay, Gaussian "scar tissue" memory (Splats), and a Niodoo physics engine patterned on LLM force dynamics.

This is not a polished framework. It's the raw, messy record of a research journey â€” from 0% success to 681/2000 wins across 4 phases over February 2026.

---

## Quick Start

```bash
# Clone
git clone https://github.com/Ruffian-L/physics-of-friendship-mountaincar-rl.git
cd physics-of-friendship-mountaincar-rl

# Install dependencies
pip install -r requirements.txt

# Run the main experiment (2000 episodes, ~10-15 min)
cd src
python main.py

# Run with visual rendering
python main.py --render

# Run the physics-only solver (2000/2000 wins, ~2 min)
python models/physics_niodoo.py
```

---

## What's in Here

```
src/
â”œâ”€â”€ main.py                 # Main training loop (2000 episodes, TDA every 5)
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ agent.py            # Q-SMA agent: Q-table + Flux + Curiosity + Dreams
â”‚   â”œâ”€â”€ tda.py              # Topological Brain: loop/void detection via Ripser
â”‚   â”œâ”€â”€ steering.py         # Steering Controller: TDA â†’ parameter adjustments
â”‚   â””â”€â”€ watcher.py          # DaydreamWatcher: background Flux landscape shaping
â””â”€â”€ models/
    â”œâ”€â”€ physics_niodoo.py   # Niodoo physics engine (LLM force vocabulary)
    â”œâ”€â”€ bridge.py           # Bodyâ†”Mind bridge (InstinctSeeder, DreamTeacher, GovernorGate)
    â”œâ”€â”€ splat_memory.py     # Gaussian volumetric reflex memory
    â””â”€â”€ niodoo.py           # Niodoo persistent memory graph

snapshots/                  # Frozen checkpoints at key milestones
â”œâ”€â”€ 2026-02-13_1456_76wins/
â”œâ”€â”€ FINAL_CHAMPION_681wins_LOG_FLUX/
â””â”€â”€ ...

research_history/           # Phase-by-phase plots and logs
â”œâ”€â”€ Phase_1_2026-02-13_TDA_Steering/
â”œâ”€â”€ Phase_2_2026-02-15_Niodoo_Dream/
â”œâ”€â”€ Phase_3_2026-02-16_Physics_Solver/
â””â”€â”€ Phase_4_2026-02-17_Splat_Bridge/

scripts/
â””â”€â”€ diagnose_well_addiction.py  # Diagnostic: visualizes the "well trap"
```

---

## The Problem

MountainCar-v0 is deceptively hard for RL. The car starts in a valley and must build momentum by swinging back and forth to reach a goal on the right hilltop. The reward is -1 per timestep (pure punishment), so the agent gets no signal about *how* to improve â€” only that it's failing. Most Q-learning agents never solve it.

---

## The Architecture: Q-SMA

**Q-SMA** stands for Q-Learning + Sensory-Motor Attunement. The core idea: an agent needs both *logic* (Q-values: what works) and *habit* (Flux: what feels natural), and must learn to transition from one to the other.

### Action Selection
```
Ï€(s) = argmax_a [ Q(s,a) + ease(F(s,a)) Ã— Î² + C(s,a) ]
```
- **Q(s,a)** â€” learned value (logic, System 2)
- **F(s,a)** â€” Flux/habit strength, passed through a sigmoid (System 1)
- **Î²** â€” confidence scaling, decays over training: `Î² = max(0.1, 1.5 Ã— 0.995^episode)`
- **C(s,a)** â€” curiosity bonus from TDA-injected attractors

### Yin-Yang Reward Shaping
The breakthrough insight. Instead of raw -1 per step, the agent receives a physics-based shaped reward:
```
R_shaped = R + Îº Ã— [Î¦(s') âˆ’ Î¦(s)]
```
where `Î¦(s) = sin(3x) + 100vÂ²` â€” a potential function that naturally creates balanced positive (gaining energy) and negative (losing energy) signals.

### TDA Metacognitive Loop
Every 5 episodes, the agent's recent trajectory is analyzed topologically:
- **Loop detection**: density heuristics + persistent homology (Ripser) detect stuck patterns
- **Void detection**: histogram analysis finds unexplored regions near the goal
- **Intervention**: decay spikes break bad habits; attractor injection encourages exploration

### Dream Cycle
Between episodes, the agent replays experiences weighted by Splat Memory proximity â€” obsessing about victories during sleep to build "neural superhighways" in the Flux landscape.

---

## Research Phases & Results

### Phase 1: TDA Steering (Feb 13) â€” 0 â†’ 76 â†’ 681 wins

The foundational work. Key breakthroughs:
1. **Yin-Yang Reward** â€” potential-based shaping that creates balanced pos/neg signals
2. **Confidence Scaling** â€” beta decay transitions habitâ†’logic
3. **Spike Cooldown** â€” prevents TDA from over-intervening

The emergent **zig-zag learning pattern** (Hot â†’ Complacent â†’ Cold â†’ Rebound) was discovered here â€” learning isn't monotonic, it oscillates upward.

<details>
<summary>ðŸ“Š Phase 1 Plots</summary>

See `research_history/Phase_1_2026-02-13_TDA_Steering/plots/` for:
- `Phase_1_-_Loop_of_Futility.png` â€” the dense spiral of being stuck
- `Phase_3_-_Post-TDA_Healing.png` â€” the opened spiral after intervention
- `zigzag_pattern.png` â€” the four-panel convergence diagnostic
- `well_addiction_diagnosis.png` â€” the "addiction to the valley bottom" trap

</details>

### Phase 2: Niodoo Dream (Feb 15) â€” 617 wins

Introduced the Niodoo physics engine and dream replay. Forces from an LLM-inspired vocabulary (Gravity Well, Repulsion, Viscosity, Adrenaline, Ghost Vector) are mapped to Mountain Car physics.

### Phase 3: Energy Pump (Feb 16) â€” 2000/2000 wins (100%)

The "push it in the direction it's going" heuristic. A trivially perfect solver (~119 steps/episode) that bypasses learning entirely. This is the well-known resonance strategy â€” it works, but it doesn't learn.

### Phase 4: The Bridge Experiments (Feb 17) â€” The Most Revealing Phase

**The question:** Can a perfect "body" (energy pump) teach a learning "mind" (Q-SMA)?

**Run 1 (Bridge 2000/2000):** Looks perfect â€” but the Governor override curve goes *up*, not down. The body does all the work. The mind never learns.

**Run 2 (Bridge 1522/2000):** The real test. Governor overrides drop to 0 at episode 1500. The mind immediately collapses to **4.4% win rate** â€” *worse* than the Q-SMA baseline of 34.1%.

> **The most important finding:** Having a perfect teacher override your decisions doesn't teach you anything. The teacher actively *prevented* learning by shielding the agent from consequences.

**Splat Memory Pivot:** 12 iterations evolving from splat reflexes in action selection (0/2000 â€” reflex spam at 3000-5000/ep overwhelmed the agent) to splats influencing only dream replay (628/2000 â€” soft curriculum through sleep).

<details>
<summary>ðŸ“Š Phase 4 Plots</summary>

See `research_history/Phase_4_2026-02-17_Splat_Bridge/plots/` for:
- `bridge_results_2026-02-17_0446.png` â€” 2000/2000 but body doing everything
- `bridge_results.png` â€” 1522/2000, the collapse at independence
- `splat_results_2026-02-17_0501.png` â€” 0/2000, reflex spam
- `splat_results.png` â€” 599/2000, the final splat iteration

</details>

---

## Hypotheses

| Hypothesis | Status | Evidence |
|:-----------|:-------|:---------|
| Yin-Yang Reward (potential-based shaping) | âœ… Confirmed | 0 â†’ 76 wins on this single change |
| System 1â†’2 Handoff (beta decay) | âœ… Confirmed | Breakthroughs cluster when beta hits floor |
| TDA Metacognitive Loop | âš ï¸ Partial | Loop detection works; unclear if Ripser adds value over density heuristic |
| Viscosity/Flow Physics Model | âŒ Failed â†’ adapted | "Correct" physics = 0 wins; naive binary = 76; log-scale = 681 |
| Splat Memory Reflexes | âš ï¸ Uncertain | Disabled in act(); only influence dreams. Untested in final form |
| Niodoo Force Vocabulary | âš ï¸ Partial | Works as standalone solver; unclear contribution in mixed system |
| Bridge (Bodyâ†’Mind) | âŒ Failed as designed | Perfect teacher prevents learning; 4.4% when independent |

---

## Key Takeaways

1. **Signal strength > physical accuracy** â€” a binary "this is good/bad" outperforms smooth physics gradients
2. **The zig-zag IS the learning** â€” oscillation between exploration and exploitation converges from above
3. **Teachers who override prevent learning** â€” the agent must face consequences to learn from them
4. **Influence dreams, not decisions** â€” soft curriculum through sleep replay works; direct reflex overrides don't

---

## Snapshots

Each snapshot in `snapshots/` contains a `TECHNICAL_WRITEUP.md` with full architectural details, math, and analysis for that point in time. Key snapshots:

- **`2026-02-13_1456_76wins/`** â€” First successful configuration. Contains the original zig-zag discovery.
- **`FINAL_CHAMPION_681wins_LOG_FLUX/`** â€” Highest-performing configuration. Contains `PHYSICS_OF_ALIGNMENT.md` exploring the "minimize resistance" hypothesis.

---

## Dependencies

- Python 3.8+
- `gymnasium>=1.0.0`
- `numpy`
- `matplotlib`
- `ripser` (for persistent homology)
- `persim`
- `scikit-learn`
- `networkx`

```bash
pip install -r requirements.txt
```

---

## About

This is the beginning of a hypothesis, not a finished paper. It's the raw record of exploring how RL agents might learn more like biological systems â€” through habit, memory, dreams, and self-correction â€” rather than pure reward maximization.

The code is messy because research is messy. The plots are real. The dead ends are documented alongside the breakthroughs.

---

## License

MIT
